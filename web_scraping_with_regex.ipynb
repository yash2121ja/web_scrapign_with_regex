{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "web scraping with regex",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0OLTLI507CU"
      },
      "source": [
        "# Import lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QWdthSIj09av"
      },
      "source": [
        "from bs4 import BeautifulSoup as soup\n",
        "from urllib.request import urlopen, Request\n",
        "import re\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WF6Ri4oC0kbb"
      },
      "source": [
        "# List of URLs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afSA7H-k0kDi"
      },
      "source": [
        "url_list = [\n",
        "            \"https://www.justdial.com/Ferozepur/search?q=engenering-college\",\n",
        "            \"https://www.justdial.com/Mumbai/Computer-Science-Colleges/nct-11008136\"\n",
        "            ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwuMPQQs0yxZ"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "# Getting whole page into mypage\n",
        "## For Email Ids"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxxka8gcsERE"
      },
      "source": [
        "email_list = []\n",
        "for i in url_list:\n",
        "  try:\n",
        "    my_url = Request(i,headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(my_url).read()\n",
        "    uClient = urlopen(my_url)\n",
        "    page_html = uClient.read()\n",
        "    uClient.close()\n",
        "    page_soup = soup(page_html, \"html.parser\")\n",
        "    mypage = str(page_soup) \n",
        "\n",
        "    #getting email\n",
        "    temp_email_list = re.findall(r'[\\w\\.-]+@[\\w\\.-]+', mypage)\n",
        "    email_list.append(temp_email_list)\n",
        "  except:\n",
        "    print(\"Error: url\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxxpKUEreHIh"
      },
      "source": [
        "## For Contact Nos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "faKXdArReHI5"
      },
      "source": [
        "contact_list = []\n",
        "for i in url_list:\n",
        "  try:\n",
        "    my_url = Request(i,headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(my_url).read()\n",
        "    uClient = urlopen(my_url)\n",
        "    page_html = uClient.read()\n",
        "    uClient.close()\n",
        "    page_soup = soup(page_html, \"html.parser\")\n",
        "    mypage = str(page_soup) \n",
        "\n",
        "    #getting email\n",
        "    temp_email_list = re.findall(r\"d{10}\", mypage)\n",
        "    email_list.append(temp_email_list)\n",
        "  except:\n",
        "    print(\"Error: url\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXuXXoGeeiOl"
      },
      "source": [
        "## For Name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0qyWOyu2eiOn"
      },
      "source": [
        "contact_list = []\n",
        "for i in url_list:\n",
        "  try:\n",
        "    my_url = Request(i,headers={'User-Agent': 'Mozilla/5.0'})\n",
        "    webpage = urlopen(my_url).read()\n",
        "    uClient = urlopen(my_url)\n",
        "    page_html = uClient.read()\n",
        "    uClient.close()\n",
        "    page_soup = soup(page_html, \"html.parser\")\n",
        "    mypage = str(page_soup) \n",
        "\n",
        "    #getting email\n",
        "    #pending\n",
        "    #temp_email_list = re.findall(r\"d{10}\", mypage)\n",
        "    email_list.append(temp_email_list)\n",
        "  except:\n",
        "    print(\"Error: url\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}